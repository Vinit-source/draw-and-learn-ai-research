{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8467c576",
   "metadata": {},
   "source": [
    "# Novel QuickDraw Solution Implementation\n",
    "\n",
    "This notebook implements the 8th place solution from the QuickDraw challenge, which takes a novel approach to processing drawing strokes.\n",
    "\n",
    "The key insight is to let the network learn features directly from the stroke data rather than rasterizing first.\n",
    "The approach uses a differentiable, trainable module to process strokes before rasterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3600567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from struct import unpack\n",
    "import collections\n",
    "import logging\n",
    "import timm  # Added timm for SEResNext model loading\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Install timm if not already installed\n",
    "try:\n",
    "    # List available SEResNeXt models in timm\n",
    "    seresnext_models = [model for model in timm.list_models() if 'seresnext' in model]\n",
    "    logger.info(\"Available SEResNeXt models in timm:\")\n",
    "    for model in seresnext_models[:5]:  # Show only first 5 models to save space\n",
    "        logger.info(f\"  - {model}\")\n",
    "    if len(seresnext_models) > 5:\n",
    "        logger.info(f\"  ... and {len(seresnext_models) - 5} more\")\n",
    "except ImportError:\n",
    "    logger.info(\"Installing timm package...\")\n",
    "    !pip install -q timm\n",
    "    import timm\n",
    "    logger.info(\"timm package installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169dc40b",
   "metadata": {},
   "source": [
    "## Data Loading Functions\n",
    "\n",
    "First, we implement functions to load and process the QuickDraw binary data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_drawing(file_handle):\n",
    "    \"\"\"Unpack a single drawing from the binary file format.\"\"\"\n",
    "    try:\n",
    "        key_id, = unpack('Q', file_handle.read(8))\n",
    "        country_code, = unpack('2s', file_handle.read(2))\n",
    "        recognized, = unpack('b', file_handle.read(1))\n",
    "        timestamp, = unpack('I', file_handle.read(4))\n",
    "        n_strokes, = unpack('H', file_handle.read(2))\n",
    "        image_strokes = []\n",
    "        for _ in range(n_strokes):\n",
    "            n_points, = unpack('H', file_handle.read(2))\n",
    "            fmt = f'{n_points}h'\n",
    "            x = unpack(fmt, file_handle.read(2 * n_points))\n",
    "            y = unpack(fmt, file_handle.read(2 * n_points))\n",
    "            image_strokes.append((x, y))\n",
    "        return {\n",
    "            'key_id': key_id,\n",
    "            'country_code': country_code,\n",
    "            'recognized': recognized,\n",
    "            'timestamp': timestamp,\n",
    "            'image': image_strokes\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error unpacking drawing: {e}\")\n",
    "        return None\n",
    "\n",
    "def unpack_drawings(filepath):\n",
    "    \"\"\"Generator that yields all drawings from a binary file.\"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                yield unpack_drawing(f)\n",
    "            except:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b08308",
   "metadata": {},
   "source": [
    "## QuickDraw Dataset Implementation\n",
    "\n",
    "We'll implement a custom PyTorch Dataset that provides both raw stroke data and rendered images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawStrokeDataset(Dataset):\n",
    "    \"\"\"Dataset that provides both stroke data and rendered images from QuickDraw binary files.\"\"\"\n",
    "    \n",
    "    IMAGE_SIZE = (256, 256)\n",
    "    LINE_WIDTH = 2\n",
    "    MAX_POINTS = 512  # Maximum number of points to consider per drawing\n",
    "    \n",
    "    def __init__(self, root, categories, transform=None, cache_size=1000):\n",
    "        self.root = root\n",
    "        self.categories = categories\n",
    "        self.transform = transform\n",
    "        self.cache_size = cache_size\n",
    "        \n",
    "        # Create mapping from category to index\n",
    "        self.category_to_idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "        \n",
    "        # Store file paths and offsets for each drawing\n",
    "        self.drawing_sources = []\n",
    "        self.drawing_cache = collections.OrderedDict()  # LRU cache\n",
    "        \n",
    "        # Load drawing offsets for each category\n",
    "        for category in categories:\n",
    "            category_name = category.replace(' ', '_')\n",
    "            filepath = os.path.join(self.root, f\"full_binary_{category_name}.bin\")\n",
    "            \n",
    "            if not os.path.exists(filepath):\n",
    "                logger.warning(f\"Dataset binary file not found: {filepath}\")\n",
    "                continue\n",
    "                \n",
    "            # Index the file to get offsets of drawings\n",
    "            offsets = []\n",
    "            offset = 0\n",
    "            with open(filepath, 'rb') as f:\n",
    "                try:\n",
    "                    drawing_count = 0\n",
    "                    while True:\n",
    "                        offsets.append(offset)\n",
    "                        # Read key_id, country_code, recognized, timestamp, n_strokes\n",
    "                        f.read(8 + 2 + 1 + 4 + 2)  \n",
    "                        n_strokes, = unpack('H', f.read(2))\n",
    "                        \n",
    "                        # For each stroke, read n_points and skip the points\n",
    "                        stroke_data_size = 0\n",
    "                        for _ in range(n_strokes):\n",
    "                            n_points, = unpack('H', f.read(2))\n",
    "                            stroke_data_size += 2 + 4 * n_points  # 2 for n_points, 4*n_points for x,y coordinates\n",
    "                            f.seek(4 * n_points, os.SEEK_CUR)  # Skip the points\n",
    "                            \n",
    "                        offset += 17 + stroke_data_size  # 17 for header, stroke_data_size for stroke data\n",
    "                        drawing_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "            logger.info(f\"Indexed {len(offsets)} drawings for category {category}\")\n",
    "            \n",
    "            # Store sources for each drawing\n",
    "            for offset in offsets:\n",
    "                self.drawing_sources.append((filepath, offset, category))\n",
    "                \n",
    "        logger.info(f\"Total number of drawings: {len(self.drawing_sources)}\")\n",
    "    \n",
    "    def _render_drawing_to_image(self, drawing_strokes):\n",
    "        \"\"\"Render stroke data to a PIL Image.\"\"\"\n",
    "        image = Image.new(\"L\", self.IMAGE_SIZE, \"white\")\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        for stroke_x, stroke_y in drawing_strokes:\n",
    "            if not stroke_x or not stroke_y:\n",
    "                continue\n",
    "                \n",
    "            if len(stroke_x) == 1:\n",
    "                draw.point((int(stroke_x[0]), int(stroke_y[0])), fill=\"black\")\n",
    "            else:\n",
    "                points = list(zip(stroke_x, stroke_y))\n",
    "                draw.line(points, fill=\"black\", width=self.LINE_WIDTH)\n",
    "                \n",
    "        return image\n",
    "    \n",
    "    def _process_strokes(self, drawing_strokes):\n",
    "        \"\"\"Process strokes into a tensor representation with (x,y,t) coordinates.\"\"\"\n",
    "        all_points = []\n",
    "        for stroke_idx, (stroke_x, stroke_y) in enumerate(drawing_strokes):\n",
    "            points = np.array([stroke_x, stroke_y]).T\n",
    "            t_values = np.linspace(0, 1, len(points))\n",
    "            \n",
    "            # Add time value as third dimension\n",
    "            points_with_time = np.column_stack([points, t_values])\n",
    "            \n",
    "            # Add stroke index information\n",
    "            stroke_info = np.ones((len(points), 1)) * stroke_idx\n",
    "            points_with_info = np.column_stack([points_with_time, stroke_info])\n",
    "            \n",
    "            all_points.append(points_with_info)\n",
    "        \n",
    "        if not all_points:\n",
    "            # Handle empty drawings\n",
    "            return np.zeros((1, 4), dtype=np.float32)\n",
    "            \n",
    "        all_points = np.vstack(all_points)\n",
    "        \n",
    "        # Sort by timestamp (t)\n",
    "        all_points = all_points[all_points[:, 2].argsort()]\n",
    "        \n",
    "        # Normalize coordinates to [0, 1]\n",
    "        all_points[:, 0] = all_points[:, 0] / 255.0\n",
    "        all_points[:, 1] = all_points[:, 1] / 255.0\n",
    "        \n",
    "        # Limit to maximum number of points\n",
    "        if len(all_points) > self.MAX_POINTS:\n",
    "            indices = np.round(np.linspace(0, len(all_points) - 1, self.MAX_POINTS)).astype(int)\n",
    "            all_points = all_points[indices]\n",
    "        \n",
    "        return all_points.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.drawing_sources)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filepath, offset, category = self.drawing_sources[idx]\n",
    "        \n",
    "        # Check if drawing is in cache\n",
    "        if idx in self.drawing_cache:\n",
    "            drawing_data = self.drawing_cache[idx]\n",
    "            self.drawing_cache.move_to_end(idx)  # Mark as recently used\n",
    "        else:\n",
    "            # Load drawing from file\n",
    "            with open(filepath, 'rb') as f:\n",
    "                f.seek(offset)\n",
    "                drawing_data = unpack_drawing(f)\n",
    "                \n",
    "            # Add to cache\n",
    "            self.drawing_cache[idx] = drawing_data\n",
    "            if len(self.drawing_cache) > self.cache_size:\n",
    "                self.drawing_cache.popitem(last=False)  # Remove oldest item\n",
    "        \n",
    "        # Process the stroke data\n",
    "        points = self._process_strokes(drawing_data['image'])\n",
    "        \n",
    "        # Render the image\n",
    "        pil_image = self._render_drawing_to_image(drawing_data['image'])\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            pil_image = self.transform(pil_image)\n",
    "        else:\n",
    "            # Convert to tensor\n",
    "            pil_image = torch.from_numpy(np.array(pil_image)).float() / 255.0\n",
    "            # Add channel dimension if grayscale\n",
    "            if pil_image.dim() == 2:\n",
    "                pil_image = pil_image.unsqueeze(0)\n",
    "        \n",
    "        # Get the category index\n",
    "        category_idx = self.category_to_idx[category]\n",
    "        \n",
    "        return {\n",
    "            'image': pil_image,\n",
    "            'points': torch.from_numpy(points),\n",
    "            'label': category_idx,\n",
    "            'category': category\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924b44cd",
   "metadata": {},
   "source": [
    "## Rasterization Module\n",
    "\n",
    "Now we implement the custom rasterization module as described in the solution. This module converts point features to an image-like representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98faf722",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointsToImage(torch.autograd.Function):\n",
    "    \"\"\"Custom autograd function to convert points with features to an image representation.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i, v):\n",
    "        \"\"\"Forward pass converting indices i and values v to a dense tensor.\n",
    "        \n",
    "        Args:\n",
    "            i: Indices tensor of shape [batch_size, 2, num_points] containing (x, y) coordinates\n",
    "            v: Values tensor of shape [batch_size, num_points, feature_size] containing features\n",
    "        \n",
    "        Returns:\n",
    "            Dense tensor of shape [batch_size, height, width, feature_size]\n",
    "        \"\"\"\n",
    "        device = i.device\n",
    "        batch_size, _, num_input_points = i.size()\n",
    "        feature_size = v.size()[2]\n",
    "\n",
    "        # Create batch indices for sparse tensor\n",
    "        batch_idx = torch.arange(batch_size, device=device).view(-1, 1).repeat(1, num_input_points).view(-1)\n",
    "        \n",
    "        # Concatenate batch, x, y indices\n",
    "        idx_full = torch.cat([batch_idx.unsqueeze(0), i.permute(1, 0, 2).contiguous().view(2, -1)], dim=0)\n",
    "        \n",
    "        # Reshape values\n",
    "        v_full = v.contiguous().view(batch_size * num_input_points, feature_size)\n",
    "        \n",
    "        # Create sparse tensor\n",
    "        if not hasattr(torch.cuda, 'sparse'):\n",
    "            # Fall back for CPU or older PyTorch versions\n",
    "            mat_dense = torch.zeros(batch_size, 32, 32, feature_size, device=device)\n",
    "            for b, x, y, idx in zip(batch_idx, idx_full[1], idx_full[2], range(len(v_full))):\n",
    "                if 0 <= x < 32 and 0 <= y < 32:\n",
    "                    mat_dense[b, x, y] += v_full[idx]\n",
    "            \n",
    "            # Count the number of points at each position\n",
    "            mat_dense_count = torch.zeros(batch_size, 32, 32, feature_size, device=device)\n",
    "            for b, x, y, idx in zip(batch_idx, idx_full[1], idx_full[2], range(len(v_full))):\n",
    "                if 0 <= x < 32 and 0 <= y < 32:\n",
    "                    mat_dense_count[b, x, y] += 1\n",
    "        else:\n",
    "            # Use sparse tensors for efficient implementation on CUDA\n",
    "            mat_sparse = torch.cuda.sparse.FloatTensor(\n",
    "                idx_full, v_full, \n",
    "                torch.Size([batch_size, 32, 32, feature_size])\n",
    "            )\n",
    "            mat_dense = mat_sparse.to_dense()\n",
    "\n",
    "            # Count the number of points at each position\n",
    "            ones_full = torch.ones(v_full.size(), device=device)\n",
    "            mat_sparse_count = torch.cuda.sparse.FloatTensor(\n",
    "                idx_full, ones_full,\n",
    "                torch.Size([batch_size, 32, 32, feature_size])\n",
    "            )\n",
    "            mat_dense_count = mat_sparse_count.to_dense()\n",
    "        \n",
    "        # Save for backward pass\n",
    "        ctx.save_for_backward(idx_full, mat_dense_count)\n",
    "        \n",
    "        # Average features at each position\n",
    "        return mat_dense / torch.clamp(mat_dense_count, 1, 1e4)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"Backward pass distributing gradients back to the original points.\"\"\"\n",
    "        idx_full, mat_dense_count = ctx.saved_tensors\n",
    "        grad_i = grad_v = None\n",
    "        \n",
    "        batch_size, _, _, feature_size = grad_output.size()\n",
    "        \n",
    "        if ctx.needs_input_grad[0]:\n",
    "            # Indices aren't differentiable\n",
    "            grad_i = None\n",
    "            \n",
    "        if ctx.needs_input_grad[1]:\n",
    "            # Get gradients at indexed positions\n",
    "            grad = grad_output[idx_full[0], idx_full[1], idx_full[2]]\n",
    "            \n",
    "            # Scale by count\n",
    "            coef = mat_dense_count[idx_full[0], idx_full[1], idx_full[2]]\n",
    "            grad_v = grad / coef\n",
    "            \n",
    "            # Reshape back to original size\n",
    "            grad_v = grad_v.view(batch_size, -1, feature_size)\n",
    "        \n",
    "        return grad_i, grad_v\n",
    "\n",
    "# Function wrapper\n",
    "points_to_image = PointsToImage.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93a3c7d",
   "metadata": {},
   "source": [
    "## Sequence Module\n",
    "\n",
    "Next, we implement the sequence module that processes point sequences using dilated convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353fa19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceModule(nn.Module):\n",
    "    \"\"\"Module for processing point sequences using dilated 1D convolutions.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=3, hidden_dim=32, output_dim=64):\n",
    "        super(SequenceModule, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, stride=1, padding=1, dilation=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=2, dilation=2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=4, dilation=4)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=8, dilation=8)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        self.conv5 = nn.Conv1d(hidden_dim, output_dim, kernel_size=2, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the sequence module.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, sequence_length, input_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape [batch_size, sequence_length, output_dim]\n",
    "        \"\"\"\n",
    "        # Transpose to channel-first format for 1D convolution\n",
    "        x = x.transpose(1, 2)  # [batch_size, input_dim, sequence_length]\n",
    "        \n",
    "        # Apply dilated convolutions\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        # Transpose back to sequence-first format\n",
    "        x = x.transpose(1, 2)  # [batch_size, sequence_length, output_dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930c1f3",
   "metadata": {},
   "source": [
    "## Full Model Implementation\n",
    "\n",
    "Now we implement the full model architecture that combines the sequence module, rasterization module, and a backbone CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91dbc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawModel(nn.Module):\n",
    "    \"\"\"Full model implementation combining sequence processing, rasterization, and CNN backbone.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, backbone='seresnext50_32x4d', pretrained=True):\n",
    "        super(QuickDrawModel, self).__init__()\n",
    "        \n",
    "        self.input_dim = 3  # x, y, t\n",
    "        self.hidden_dim = 32\n",
    "        self.feature_dim = 64\n",
    "        \n",
    "        # Sequence module to process point sequences\n",
    "        self.sequence_module = SequenceModule(\n",
    "            input_dim=self.input_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            output_dim=self.feature_dim\n",
    "        )\n",
    "        \n",
    "        # Initialize backbone CNN using timm\n",
    "        self.backbone_name = backbone\n",
    "        \n",
    "        # Use timm to load the SEResNeXt model\n",
    "        if 'seresnext' in backbone or 'senet' in backbone:\n",
    "            # Load pretrained SEResNeXt from timm\n",
    "            self.backbone = timm.create_model(backbone, pretrained=pretrained)\n",
    "            \n",
    "            # Get the number of input channels in the first convolutional layer\n",
    "            if hasattr(self.backbone, 'conv1'):\n",
    "                # For ResNet-like models\n",
    "                original_conv1 = self.backbone.conv1\n",
    "                self.backbone.conv1 = nn.Conv2d(\n",
    "                    self.feature_dim,  # Set input channels to our feature dimension\n",
    "                    original_conv1.out_channels, \n",
    "                    kernel_size=original_conv1.kernel_size,\n",
    "                    stride=original_conv1.stride,\n",
    "                    padding=original_conv1.padding,\n",
    "                    bias=False if original_conv1.bias is None else True\n",
    "                )\n",
    "            elif hasattr(self.backbone, 'conv_stem'):\n",
    "                # For EfficientNet-like models\n",
    "                original_conv = self.backbone.conv_stem\n",
    "                self.backbone.conv_stem = nn.Conv2d(\n",
    "                    self.feature_dim,\n",
    "                    original_conv.out_channels,\n",
    "                    kernel_size=original_conv.kernel_size,\n",
    "                    stride=original_conv.stride,\n",
    "                    padding=original_conv.padding,\n",
    "                    bias=False if original_conv.bias is None else True\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model architecture: {backbone}. Cannot find first conv layer.\")\n",
    "            \n",
    "            # Replace final classifier\n",
    "            if hasattr(self.backbone, 'fc'):\n",
    "                in_features = self.backbone.fc.in_features\n",
    "                self.backbone.fc = nn.Linear(in_features, num_classes)\n",
    "            elif hasattr(self.backbone, 'classifier'):\n",
    "                if isinstance(self.backbone.classifier, nn.Linear):\n",
    "                    in_features = self.backbone.classifier.in_features\n",
    "                    self.backbone.classifier = nn.Linear(in_features, num_classes)\n",
    "                else:\n",
    "                    # Some models have a more complex classifier\n",
    "                    in_features = self.backbone.classifier.in_features \n",
    "                    if hasattr(self.backbone.classifier, 'in_features'):\n",
    "                        in_features = self.backbone.classifier.in_features\n",
    "                    elif hasattr(self.backbone.classifier, 'fc') and hasattr(self.backbone.classifier.fc, 'in_features'):\n",
    "                        in_features = self.backbone.classifier.fc.in_features\n",
    "                    else:\n",
    "                        raise ValueError(f\"Cannot determine input features for classifier in {backbone}\")\n",
    "                    \n",
    "                    self.backbone.classifier = nn.Linear(in_features, num_classes)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model architecture: {backbone}. Cannot find classifier.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone}. Please use a SEResNeXt model from timm.\")\n",
    "    \n",
    "    def forward(self, points):\n",
    "        \"\"\"Forward pass through the full model.\n",
    "        \n",
    "        Args:\n",
    "            points: Points tensor of shape [batch_size, num_points, 3] containing (x, y, t) coordinates\n",
    "            \n",
    "        Returns:\n",
    "            Class logits\n",
    "        \"\"\"\n",
    "        batch_size = points.size(0)\n",
    "        \n",
    "        # Process point sequences with sequence module\n",
    "        # Use only the first 3 dimensions (x, y, t) and ignore stroke_idx if present\n",
    "        point_features = self.sequence_module(points[:, :, :3])\n",
    "        \n",
    "        # Prepare indices for rasterization\n",
    "        # Scale x,y coordinates to [0, 31] for a 32x32 grid\n",
    "        indices = points[:, :, :2].clone()\n",
    "        indices = torch.clamp(indices * 31, 0, 31).long()\n",
    "        \n",
    "        # Rasterize points to image\n",
    "        feature_image = points_to_image(indices, point_features)\n",
    "        \n",
    "        # Reorder dimensions for CNN: [batch, feature, height, width]\n",
    "        feature_image = feature_image.permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        # Pass through backbone CNN\n",
    "        logits = self.backbone(feature_image)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1005aa",
   "metadata": {},
   "source": [
    "## Inference Implementation\n",
    "\n",
    "Now let's implement functions to load the dataset and run inference on the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab924801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categories we want to use\n",
    "QUICKDRAW_CATEGORIES = ['apple', 'banana', 'car', 'cat', 'dog']\n",
    "DATA_ROOT = './data'\n",
    "\n",
    "# Create the dataset\n",
    "def create_dataset(categories=QUICKDRAW_CATEGORIES, root=DATA_ROOT, batch_size=32):\n",
    "    dataset = QuickDrawStrokeDataset(root=root, categories=categories)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    return dataset, dataloader\n",
    "\n",
    "# Initialize the model\n",
    "def create_model(num_classes=len(QUICKDRAW_CATEGORIES), backbone='seresnext50_32x4d'):\n",
    "    model = QuickDrawModel(num_classes=num_classes, backbone=backbone, pretrained=True)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset, dataloader = create_dataset()\n",
    "logger.info(f\"Dataset loaded with {len(dataset)} samples from {len(QUICKDRAW_CATEGORIES)} categories\")\n",
    "\n",
    "# Create the model\n",
    "model = create_model()\n",
    "logger.info(f\"Model created with backbone: seresnext50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e419135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "def visualize_samples(dataset, num_samples=5):\n",
    "    fig, axes = plt.subplots(num_samples, 1, figsize=(10, 4 * num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        idx = np.random.randint(0, len(dataset))\n",
    "        sample = dataset[idx]\n",
    "        \n",
    "        # Display the image\n",
    "        if sample['image'].dim() == 3:\n",
    "            # Convert from CHW to HWC for display\n",
    "            img = sample['image'].permute(1, 2, 0).numpy()\n",
    "            if img.shape[2] == 1:  # If grayscale\n",
    "                img = img.squeeze(2)\n",
    "        else:\n",
    "            img = sample['image'].numpy()\n",
    "        \n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f\"Category: {sample['category']}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a few samples\n",
    "visualize_samples(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33665e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on a batch\n",
    "def run_inference(model, dataloader, num_batches=1):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            if batch_idx >= num_batches:\n",
    "                break\n",
    "                \n",
    "            points = batch['points'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(points)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, preds = torch.max(probs, 1)\n",
    "            \n",
    "            # Store results\n",
    "            for i in range(len(labels)):\n",
    "                results.append({\n",
    "                    'true_label': labels[i].item(),\n",
    "                    'pred_label': preds[i].item(),\n",
    "                    'true_category': batch['category'][i],\n",
    "                    'probabilities': probs[i].cpu().numpy()\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test inference on a few batches\n",
    "try:\n",
    "    inference_results = run_inference(model, dataloader, num_batches=1)\n",
    "    logger.info(f\"Inference completed on {len(inference_results)} samples\")\n",
    "    \n",
    "    # logger.info some sample results\n",
    "    for i, result in enumerate(inference_results[:5]):\n",
    "        logger.info(f\"Sample {i}:\")\n",
    "        logger.info(f\"  True category: {result['true_category']}\")\n",
    "        logger.info(f\"  Predicted category: {QUICKDRAW_CATEGORIES[result['pred_label']]}\")\n",
    "        logger.info(f\"  Probabilities: {result['probabilities']}\")\n",
    "        logger.info()\n",
    "except Exception as e:\n",
    "    logger.info(f\"Error during inference: {e}\")\n",
    "    logger.info(\"Note: The model requires training before it can generate meaningful predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96713d5",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Here's how you would train the model (not executed in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs=10, learning_rate=1e-4):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            points = batch['points'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(points)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                logger.info(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(dataloader)}, \"\n",
    "                      f\"Loss: {loss.item():.4f}, Acc: {100 * correct / total:.2f}%\")\n",
    "        \n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(dataloader):.4f}, \"\n",
    "              f\"Acc: {100 * correct / total:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# To train the model, uncomment and run the following:\n",
    "# trained_model = train_model(model, dataloader, num_epochs=5)\n",
    "# torch.save(trained_model.state_dict(), 'quickdraw_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014baa89",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook implemented the 8th place solution for the QuickDraw dataset challenge. The approach combines:\n",
    "\n",
    "1. A sequence module using dilated 1D convolutions to process stroke data\n",
    "2. A custom rasterization module to convert strokes to image-like representations\n",
    "3. A modified ResNet architecture to perform the classification\n",
    "\n",
    "Key innovations in this approach:\n",
    "- The use of differentiable rasterization to bridge between vector and raster representations\n",
    "- The effective use of both temporal and spatial information\n",
    "- A phased training approach that gradually unfreezes pre-trained weights\n",
    "\n",
    "This architecture demonstrates how combining different data representations (strokes and images) can lead to improved performance on sketch recognition tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
