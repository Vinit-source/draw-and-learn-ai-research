{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install() # This will restart the kernel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu93MLm8NcwB",
        "outputId": "d46ff7c2-d58c-4022-b8a1-75e7961a2899"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è¨ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "üì¶ Installing...\n",
            "üìå Adjusting configuration...\n",
            "ü©π Patching environment...\n",
            "‚è≤ Done in 0:00:11\n",
            "üîÅ Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda create -n myenv python=3.9 -y\n",
        "# condacolab usually activates the base environment by default.\n",
        "# To activate your specific environment for subsequent ! commands in the session:\n",
        "# This is tricky because each ! is a new shell.\n",
        "# condacolab aims to make the *kernel itself* run within a Conda environment.\n",
        "# Check active environment:\n",
        "!conda env list\n",
        "!python --version # Should show the python from the active conda env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuHj1NoaqKiT",
        "outputId": "7774ee28-5958-4898-d1d7-3e1381aadaf1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bfailed\n",
            "\n",
            "CondaError: KeyboardInterrupt\n",
            "\n",
            "\n",
            "# conda environments:\n",
            "#\n",
            "base                   /usr/local\n",
            "\n",
            "Python 3.11.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhSD1TxIvTFD",
        "outputId": "6c23f6ce-487a-4c9c-adfc-e5af01ba43ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ],
      "metadata": {
        "id": "IMFP9BfuvkUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ai-edge-torch"
      ],
      "metadata": {
        "id": "YggipOVcv6rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooW8zqniMsrK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import urllib.parse\n",
        "\n",
        "# Part 1: Download QuickDraw Dataset Subsample\n",
        "\n",
        "# --- Configuration for QuickDraw Download ---\n",
        "# Please expand this list with up to 50 distinct object categories\n",
        "# relevant to Indian primary education (Standards 1st to 5th).\n",
        "# Ensure words are common and visually distinct.\n",
        "QUICKDRAW_CATEGORIES = [\n",
        "    'apple', 'cat', 'dog', 'door', 'elephant', 'fish', 'flower', 'grapes',\n",
        "    'grass', 'house', 'ice cream', 'jail', 'key', 'lion', 'moon', 'nose',\n",
        "    'pencil', 'rabbit', 'sun', 'tree', 'umbrella', 'van', 'cake', 'airplane',\n",
        "    'ant', 'banana', 'bed', 'bee', 'bicycle', 'bird', 'book', 'bread', 'bus',\n",
        "    'elbow', 'ear', 'camera', 'car', 'chair', 'clock', 'cloud', 'hand',\n",
        "    'computer', 'cookie', 'cow', 'crayon', 'cup', 'eraser', 'carrot', 'drums',\n",
        "    'eye', 'knife'\n",
        "]\n",
        "QUICKDRAW_BASE_URL = \"https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/\"\n",
        "DOWNLOAD_DIR = \"./quickdraw_data/\"\n",
        "\n",
        "def download_quickdraw_subset(categories, base_url, download_dir):\n",
        "    \"\"\"\n",
        "    Downloads .npy files for specified QuickDraw categories.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(download_dir):\n",
        "        os.makedirs(download_dir)\n",
        "        print(f\"Created directory: {download_dir}\")\n",
        "\n",
        "    print(f\"Starting download of {len(categories)} QuickDraw categories...\")\n",
        "    for category in categories:\n",
        "        # Sanitize category name for URL (replace spaces with %20)\n",
        "        sanitized_category_name = urllib.parse.quote(category)\n",
        "        file_name = f\"{sanitized_category_name}.npy\"\n",
        "        url = f\"{base_url}{file_name}\"\n",
        "        output_path = os.path.join(download_dir, f\"{category.replace(' ', '_')}.npy\") # Use underscore for local filename\n",
        "\n",
        "        if os.path.exists(output_path):\n",
        "            print(f\"File for '{category}' already exists at {output_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Downloading '{category}' from {url}...\")\n",
        "        try:\n",
        "            response = requests.get(url, stream=True)\n",
        "            response.raise_for_status()  # Raise an exception for HTTP errors (4xx or 5xx)\n",
        "\n",
        "            with open(output_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            print(f\"Successfully downloaded and saved to {output_path}\")\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if e.response.status_code == 404:\n",
        "                print(f\"Error: Category '{category}' not found (404) at {url}. Skipping.\")\n",
        "            else:\n",
        "                print(f\"Error downloading '{category}': {e}. Skipping.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading '{category}': {e}. Skipping.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred for '{category}': {e}. Skipping.\")\n",
        "    print(\"QuickDraw download process finished.\")\n",
        "\n",
        "# Part 2: Proof-of-Concept (POC) SeResNext to .tflite Conversion using AI Edge Torch\n",
        "\n",
        "def convert_seresnext_to_tflite():\n",
        "    \"\"\"\n",
        "    Loads a pretrained SeResNext model, converts it to .tflite using AI Edge Torch,\n",
        "    and optionally verifies the conversion.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import torch\n",
        "        import timm\n",
        "        import ai_edge_torch\n",
        "        import numpy as np # For dummy input and TFLite verification\n",
        "        import tensorflow as tf # For TFLite verification\n",
        "    except ImportError as e:\n",
        "        print(f\"ImportError: {e}. Please ensure all required libraries are installed.\")\n",
        "        print(\"Installation commands:\")\n",
        "        print(\"  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\")\n",
        "        print(\"  pip install timm\")\n",
        "        print(\"  pip install ai-edge-torch\")\n",
        "        print(\"  pip install tensorflow numpy\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nStarting SeResNext to .tflite conversion...\")\n",
        "\n",
        "    # 1. Load Pretrained SeResNext Model\n",
        "    # We'll use 'seresnext50_32x4d' as a common variant.\n",
        "    # Its default input size is (3, 224, 224).\n",
        "    model_name = 'seresnext50_32x4d'\n",
        "    input_height, input_width = 224, 224\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading pretrained model: {model_name}\")\n",
        "        model = timm.create_model(model_name, pretrained=True)\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        print(f\"Model {model_name} loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model with timm: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Prepare Dummy Input\n",
        "    # Input shape (batch_size, channels, height, width)\n",
        "    dummy_input = torch.randn(1, 3, input_height, input_width)\n",
        "    print(f\"Prepared dummy input with shape: {dummy_input.shape}\")\n",
        "\n",
        "    # 3. Convert to .tflite with AI Edge Torch\n",
        "    print(\"Converting model to AI Edge Torch format...\")\n",
        "    try:\n",
        "        # The second argument to convert must be a tuple of example inputs.\n",
        "        edge_model = ai_edge_torch.convert(model, (dummy_input,))\n",
        "        print(\"Model converted to Edge format.\")\n",
        "\n",
        "        # Export to TFLite flatbuffer\n",
        "        tflite_model_path = \"./content/seresnext_model.tflite\"\n",
        "        edge_model.export(tflite_model_path)\n",
        "        print(\"Model exported to file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during AI Edge Torch conversion: {e}\")\n",
        "        return\n",
        "\n",
        "    # 5. Verification (Optional but Recommended)\n",
        "    print(\"\\nVerifying the .tflite model...\")\n",
        "    try:\n",
        "        interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "        interpreter.allocate_tensors()\n",
        "        print(\"TFLite model loaded into interpreter.\")\n",
        "\n",
        "        input_details = interpreter.get_input_details()\n",
        "        output_details = interpreter.get_output_details()\n",
        "\n",
        "        print(f\"  Input details: {input_details[0]['name']}, Shape: {input_details[0]['shape']}, Dtype: {input_details[0]['dtype']}\")\n",
        "        print(f\"  Output details: {output_details[0]['name']}, Shape: {output_details[0]['shape']}, Dtype: {output_details[0]['dtype']}\")\n",
        "\n",
        "        # Prepare input for TFLite model.\n",
        "        # PyTorch uses NCHW (Batch, Channels, Height, Width).\n",
        "        # TFLite models converted by ai_edge_torch often expect NHWC (Batch, Height, Width, Channels).\n",
        "        # Let's check the expected shape.\n",
        "        expected_shape = input_details[0]['shape']\n",
        "        input_data_np = dummy_input.numpy()\n",
        "\n",
        "        if expected_shape.tolist() == [1, input_height, input_width, 3]: # NHWC\n",
        "            print(\"TFLite model expects NHWC input. Transposing dummy input.\")\n",
        "            input_data_np = np.transpose(input_data_np, (0, 2, 3, 1)) # NCHW to NHWC\n",
        "        elif expected_shape.tolist() == [1, 3, input_height, input_width]: # NCHW\n",
        "            print(\"TFLite model expects NCHW input. No transpose needed.\")\n",
        "        else:\n",
        "            print(f\"Warning: TFLite input shape {expected_shape} is unexpected. Using original dummy input shape.\")\n",
        "            # May need adjustment if this warning appears.\n",
        "\n",
        "        # Ensure dtype matches\n",
        "        if input_details[0]['dtype'] == np.float32 and input_data_np.dtype != np.float32:\n",
        "            input_data_np = input_data_np.astype(np.float32)\n",
        "\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_data_np)\n",
        "        interpreter.invoke()\n",
        "        output_data_tflite = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "        print(f\"TFLite model inference successful. Output shape: {output_data_tflite.shape}\")\n",
        "        # print(f\"Sample output values (first 5): {output_data_tflite.flatten()[:5]}\")\n",
        "\n",
        "        # For a more thorough check, you could compare PyTorch output with TFLite output\n",
        "        with torch.no_grad():\n",
        "            pytorch_output = model(dummy_input)\n",
        "        pytorch_output_np = pytorch_output.numpy()\n",
        "        np.testing.assert_allclose(pytorch_output_np, output_data_tflite, rtol=1e-3, atol=1e-5)\n",
        "        print(\"PyTorch and TFLite outputs are close (basic check).\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during TFLite model verification: {e}\")\n",
        "\n",
        "    print(\"SeResNext to .tflite conversion and verification process finished.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Part 1 Execution ---\n",
        "    # You can adjust the QUICKDRAW_CATEGORIES list above before running.\n",
        "    # Note: Downloading many categories can take time and disk space.\n",
        "    # download_quickdraw_subset(QUICKDRAW_CATEGORIES, QUICKDRAW_BASE_URL, DOWNLOAD_DIR)\n",
        "    print(\"Part 1 (QuickDraw Download) is commented out by default to prevent accidental large downloads.\")\n",
        "    print(\"Uncomment the line 'download_quickdraw_subset(...)' in the if __name__ == '__main__': block to run it.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # --- Part 2 Execution ---\n",
        "    convert_seresnext_to_tflite()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SeResNet-50 using the following configs:\n",
        "GitHub Copilot\n",
        "This is a significant update that involves setting up a complete fine-tuning pipeline. Below is a Python script that outlines the major components.\n",
        "\n",
        "Important Considerations and Assumptions:\n",
        "\n",
        "Stroke Data for 3-Channel Input:\n",
        "\n",
        "The QuickDraw .npy files you're downloading (numpy_bitmap/) are 28x28 grayscale bitmaps. They do not contain the raw stroke data required to create channels from 1/3, 2/3, and 3/3 of strokes.\n",
        "Adaptation Made: To proceed with the downloaded .npy files, this script will resize the 28x28 grayscale image to 224x224 and then replicate this single channel to create a 3-channel image. This is a common workaround when fine-tuning models pre-trained on RGB images (like ImageNet models) with grayscale data.\n",
        "If you strictly need the stroke-based 3-channel input, you would need to download the .ndjson files (raw stroke data) and implement a more complex preprocessing pipeline to render images from partial strokes.\n",
        "\"Unrecognized\" Class:\n",
        "\n",
        "The prompt mentions \"including unrecognized.\" The QuickDraw dataset doesn't have a generic \"unrecognized\" class. You would need to select one of the existing 345 classes to serve this purpose (e.g., \"The Mona Lisa\" is sometimes used) or curate your own \"unrecognized\" samples. This script will proceed with the 50 specified classes. If you want an \"unrecognized\" category, add its name to QUICKDRAW_CATEGORIES and ensure its .npy file is downloaded.\n",
        "MAP@3 for Learning Rate Scheduler:\n",
        "\n",
        "Calculating Mean Average Precision at 3 (MAP@3) typically requires the model to output a ranked list of predictions and is more common in retrieval or ranking tasks. For a standard classifier outputting logits for 50 classes, it's simpler to use overall validation accuracy or loss.\n",
        "Adaptation Made: This script will use validation accuracy to monitor for the ReduceLROnPlateau scheduler. If MAP@3 is a hard requirement, the evaluation logic and potentially the model's output layer would need significant adjustments.\n",
        "Averaging Last Ten Weights & TTA for Inference:\n",
        "\n",
        "The script will implement saving checkpoints.\n",
        "The logic for averaging the last ten weights and performing horizontal flip Test-Time Augmentation (TTA) will be outlined conceptually for the inference phase, as a full inference script is extensive. The TFLite conversion will typically be done on a single (potentially weight-averaged) fine-tuned model.\n",
        "Computational Resources: Fine-tuning seresnext50_32x4d with a batch size of 256 requires a capable GPU with sufficient VRAM."
      ],
      "metadata": {
        "id": "pduPiVl8_D2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import urllib.parse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "from sklearn.model_selection import train_test_split # For splitting indices\n",
        "import random\n",
        "import glob # For finding checkpoints\n",
        "\n",
        "# --- Configuration ---\n",
        "QUICKDRAW_CATEGORIES = [ # Your 50 classes\n",
        "    'apple', 'ball', 'cat', 'dog', 'elephant',\n",
        "    'fish', 'grapes', 'house', 'ice cream', 'jellyfish',\n",
        "    'kite', 'lion', 'moon', 'nose', 'orange',\n",
        "    'pencil', 'question mark', 'rabbit', 'sun', 'tree',\n",
        "    'umbrella', 'van', 'watch', 'xylophone', 'yo-yo',\n",
        "    'zebra', 'airplane', 'ant', 'banana', 'bed',\n",
        "    'bee', 'bicycle', 'bird', 'boat', 'book',\n",
        "    'bread', 'bus', 'butterfly', 'cake', 'camera',\n",
        "    'car', 'chair', 'clock', 'cloud', 'computer',\n",
        "    'cookie', 'cow', 'crayon', 'cup', 'eraser'\n",
        "]\n",
        "NUM_CLASSES = len(QUICKDRAW_CATEGORIES)\n",
        "QUICKDRAW_BASE_URL = \"https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/\"\n",
        "DOWNLOAD_DIR = \"./quickdraw_data/\"\n",
        "CHECKPOINT_DIR = \"./quickdraw_checkpoints/\"\n",
        "MODEL_NAME = 'seresnext50_32x4d'\n",
        "INPUT_SIZE = 224\n",
        "BATCH_SIZE = 256 # Adjust if OOM error occurs\n",
        "LEARNING_RATE = 0.00025\n",
        "NUM_EPOCHS = 20 # Adjust as needed\n",
        "VALIDATION_SAMPLES_PER_CLASS = 500\n",
        "CHECKPOINT_SAVE_STEP = 5000 # Save every 5000 training steps\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "if not os.path.exists(CHECKPOINT_DIR):\n",
        "    os.makedirs(CHECKPOINT_DIR)\n",
        "\n",
        "# --- Part 1: Data Download and Preparation ---\n",
        "def download_quickdraw_subset(categories, base_url, download_dir):\n",
        "    if not os.path.exists(download_dir):\n",
        "        os.makedirs(download_dir)\n",
        "    print(f\"Starting download of {len(categories)} QuickDraw categories...\")\n",
        "    for category in categories:\n",
        "        sanitized_category_name = urllib.parse.quote(category)\n",
        "        file_name = f\"{sanitized_category_name}.npy\"\n",
        "        url = f\"{base_url}{file_name}\"\n",
        "        output_path = os.path.join(download_dir, f\"{category.replace(' ', '_')}.npy\")\n",
        "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
        "            print(f\"File for '{category}' already exists. Skipping.\")\n",
        "            continue\n",
        "        print(f\"Downloading '{category}' from {url}...\")\n",
        "        try:\n",
        "            response = requests.get(url, stream=True)\n",
        "            response.raise_for_status()\n",
        "            with open(output_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192): f.write(chunk)\n",
        "            print(f\"Successfully downloaded and saved to {output_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading '{category}': {e}. Skipping.\")\n",
        "    print(\"QuickDraw download process finished.\")\n",
        "\n",
        "class QuickDrawBitmapDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, class_to_idx, transform=None):\n",
        "        self.image_paths = image_paths # List of (filepath, index_in_npy)\n",
        "        self.labels = labels         # List of integer labels\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "        self._loaded_npy_files = {} # Cache for loaded npy files\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def _load_image(self, filepath, index_in_npy):\n",
        "        if filepath not in self._loaded_npy_files:\n",
        "            try:\n",
        "                self._loaded_npy_files[filepath] = np.load(filepath, mmap_mode='r')\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {filepath}: {e}\")\n",
        "                return None # Or raise error\n",
        "\n",
        "        try:\n",
        "            # Images are 784 (28*28) flat arrays, normalized to 0-1\n",
        "            img = self._loaded_npy_files[filepath][index_in_npy].astype(np.float32)\n",
        "            img = img.reshape(28, 28)\n",
        "        except IndexError:\n",
        "            print(f\"Index {index_in_npy} out of bounds for {filepath} (len {len(self._loaded_npy_files[filepath])}). Skipping sample.\")\n",
        "            return None # Skip this sample\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {index_in_npy} from {filepath}: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Create 3-channel image by resizing and replicating the grayscale channel\n",
        "        # This adapts the 28x28 grayscale to the 224x224x3 expected by the model\n",
        "        img_tensor = torch.from_numpy(img).unsqueeze(0) # (1, 28, 28)\n",
        "\n",
        "        # Resize to INPUT_SIZE x INPUT_SIZE\n",
        "        resize_transform = transforms.Resize((INPUT_SIZE, INPUT_SIZE), antialias=True)\n",
        "        img_resized = resize_transform(img_tensor) # (1, INPUT_SIZE, INPUT_SIZE)\n",
        "\n",
        "        # Replicate to 3 channels\n",
        "        img_3_channel = img_resized.repeat(3, 1, 1) # (3, INPUT_SIZE, INPUT_SIZE)\n",
        "\n",
        "        # Normalize if necessary (timm models usually handle this)\n",
        "        # For ImageNet, normalization is typically:\n",
        "        # normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        # img_3_channel = normalize(img_3_channel)\n",
        "        # However, timm models often have built-in normalization or expect 0-1 input.\n",
        "        # For QuickDraw, images are already 0-1. Replicating keeps them 0-1.\n",
        "\n",
        "        if self.transform: # For TTA later, not used in training as per \"no augmentation\"\n",
        "            img_3_channel = self.transform(img_3_channel)\n",
        "\n",
        "        return img_3_channel\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filepath, index_in_npy = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        image = self._load_image(filepath, index_in_npy)\n",
        "        if image is None: # Handle cases where image loading failed\n",
        "            # Return a dummy sample or skip. For simplicity, we might need a collate_fn to handle Nones\n",
        "            # Or ensure _load_image always returns a valid tensor (e.g. a black image)\n",
        "            print(f\"Warning: Could not load image for idx {idx}, path {filepath}, npy_idx {index_in_npy}. Returning dummy data.\")\n",
        "            return torch.zeros((3, INPUT_SIZE, INPUT_SIZE)), torch.tensor(0) # Dummy data\n",
        "\n",
        "        return image, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "def prepare_dataloaders(categories, download_dir, val_samples_per_class, batch_size):\n",
        "    class_to_idx = {name: i for i, name in enumerate(categories)}\n",
        "    idx_to_class = {i: name for i, name in enumerate(categories)}\n",
        "\n",
        "    all_image_paths = [] # List of (filepath, index_in_npy)\n",
        "    all_labels = []\n",
        "\n",
        "    for class_name in categories:\n",
        "        npy_path = os.path.join(download_dir, f\"{class_name.replace(' ', '_')}.npy\")\n",
        "        if not os.path.exists(npy_path):\n",
        "            print(f\"Warning: NPY file not found for {class_name} at {npy_path}. Skipping this class.\")\n",
        "            continue\n",
        "        try:\n",
        "            # Load just to get the count, actual loading in Dataset\n",
        "            data = np.load(npy_path, mmap_mode='r')\n",
        "            num_images_in_class = data.shape[0]\n",
        "            del data # Free memory\n",
        "\n",
        "            indices = list(range(num_images_in_class))\n",
        "            random.shuffle(indices)\n",
        "\n",
        "            val_indices_class = indices[:val_samples_per_class]\n",
        "            train_indices_class = indices[val_samples_per_class:]\n",
        "\n",
        "            for i in train_indices_class:\n",
        "                all_image_paths.append((npy_path, i))\n",
        "                all_labels.append(class_to_idx[class_name])\n",
        "            for i in val_indices_class:\n",
        "                all_image_paths.append((npy_path, i)) # Add to all_image_paths for splitting later\n",
        "                all_labels.append(class_to_idx[class_name])\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing class {class_name} from {npy_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not all_image_paths:\n",
        "        print(\"No data loaded. Exiting.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Split all_image_paths and all_labels into train and val sets\n",
        "    # This ensures that validation samples are strictly from the designated val_indices_class\n",
        "    train_img_paths, val_img_paths, train_labels, val_labels = [], [], [], []\n",
        "\n",
        "    temp_class_val_counts = {cls_idx: 0 for cls_idx in class_to_idx.values()}\n",
        "\n",
        "    # Create a combined list of (path_tuple, label) to shuffle for train/val split\n",
        "    # This is a bit complex because we pre-selected val_samples_per_class *from each class*\n",
        "    # A simpler way is to iterate through classes, assign val, then train.\n",
        "\n",
        "    # Let's rebuild train/val lists directly\n",
        "    for class_name in categories:\n",
        "        class_idx = class_to_idx[class_name]\n",
        "        npy_path = os.path.join(download_dir, f\"{class_name.replace(' ', '_')}.npy\")\n",
        "        if not os.path.exists(npy_path): continue\n",
        "\n",
        "        try:\n",
        "            data_len = np.load(npy_path, mmap_mode='r').shape[0]\n",
        "            indices = list(range(data_len))\n",
        "            random.shuffle(indices)\n",
        "\n",
        "            current_val_indices = indices[:val_samples_per_class]\n",
        "            current_train_indices = indices[val_samples_per_class:]\n",
        "\n",
        "            for i in current_train_indices:\n",
        "                train_img_paths.append((npy_path, i))\n",
        "                train_labels.append(class_idx)\n",
        "            for i in current_val_indices:\n",
        "                val_img_paths.append((npy_path, i))\n",
        "                val_labels.append(class_idx)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during train/val split for {class_name}: {e}\")\n",
        "\n",
        "\n",
        "    print(f\"Total training samples: {len(train_img_paths)}\")\n",
        "    print(f\"Total validation samples: {len(val_img_paths)}\")\n",
        "\n",
        "    if not train_img_paths or not val_img_paths:\n",
        "        print(\"Not enough data for training or validation split. Exiting.\")\n",
        "        return None, None, None\n",
        "\n",
        "    train_dataset = QuickDrawBitmapDataset(train_img_paths, train_labels, class_to_idx)\n",
        "    val_dataset = QuickDrawBitmapDataset(val_img_paths, val_labels, class_to_idx)\n",
        "\n",
        "    # Custom collate_fn to filter out None values if image loading fails\n",
        "    def collate_fn_skip_none(batch):\n",
        "        batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "        if not batch: # If all samples in batch failed\n",
        "            return torch.empty(0), torch.empty(0)\n",
        "        return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn_skip_none)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_fn_skip_none)\n",
        "\n",
        "    return train_loader, val_loader, class_to_idx\n",
        "\n",
        "# --- Part 2: Model Definition ---\n",
        "def get_model(num_classes, model_name=MODEL_NAME, pretrained=True):\n",
        "    model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
        "    # Example: For seresnext50_32x4d, the classifier is model.fc\n",
        "    # For other models, it might be model.classifier or model.head\n",
        "    # print(model) # To inspect the model structure and find the classifier layer name\n",
        "    # Assuming it's 'fc' for seresnext50_32x4d\n",
        "    # if hasattr(model, 'fc') and isinstance(model.fc, nn.Linear):\n",
        "    #     num_ftrs = model.fc.in_features\n",
        "    #     model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    # elif hasattr(model, 'classifier') and isinstance(model.classifier, nn.Linear):\n",
        "    #     num_ftrs = model.classifier.in_features\n",
        "    #     model.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "    # else:\n",
        "    #     print(\"Warning: Could not automatically find and replace the classifier layer.\")\n",
        "    # timm's create_model with num_classes argument usually handles this.\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "# --- Part 3: Training Loop ---\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, checkpoint_dir, checkpoint_save_step):\n",
        "    best_val_acc = 0.0\n",
        "    global_step = 0\n",
        "    recent_checkpoints = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            if inputs.numel() == 0: # Skip if batch is empty due to collate_fn\n",
        "                continue\n",
        "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "            global_step += 1\n",
        "            if global_step % checkpoint_save_step == 0:\n",
        "                checkpoint_path = os.path.join(checkpoint_dir, f\"model_step_{global_step}.pth\")\n",
        "                torch.save(model.state_dict(), checkpoint_path)\n",
        "                print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "                recent_checkpoints.append(checkpoint_path)\n",
        "                if len(recent_checkpoints) > 10: # Keep only the last 10\n",
        "                    oldest_ckpt = recent_checkpoints.pop(0)\n",
        "                    if os.path.exists(oldest_ckpt):\n",
        "                        # os.remove(oldest_ckpt) # Optional: remove older checkpoints\n",
        "                        pass\n",
        "\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc_train = correct_train / total_train if total_train > 0 else 0\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                if inputs.numel() == 0: continue\n",
        "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = val_loss / len(val_loader.dataset) if len(val_loader.dataset) > 0 else 0\n",
        "        epoch_val_acc = correct_val / total_val if total_val > 0 else 0\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Train Loss: {epoch_loss:.4f} Acc: {epoch_acc_train:.4f} | \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step(epoch_val_acc) # ReduceLROnPlateau expects a metric\n",
        "\n",
        "        if epoch_val_acc > best_val_acc:\n",
        "            best_val_acc = epoch_val_acc\n",
        "            best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"New best model saved to {best_model_path} with Val Acc: {best_val_acc:.4f}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    return recent_checkpoints # Return list of last 10 checkpoints\n",
        "\n",
        "# --- Part 4: Inference Concepts (Not a full script) ---\n",
        "def average_last_k_weights(checkpoint_paths):\n",
        "    if not checkpoint_paths:\n",
        "        print(\"No checkpoints provided for averaging.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Averaging weights from: {checkpoint_paths}\")\n",
        "    avg_state_dict = None\n",
        "\n",
        "    # Load the first checkpoint to initialize avg_state_dict\n",
        "    try:\n",
        "        base_state_dict = torch.load(checkpoint_paths[0], map_location=DEVICE)\n",
        "        avg_state_dict = {k: v.clone().to(dtype=torch.float32) for k, v in base_state_dict.items()} # Ensure float for accumulation\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading base checkpoint {checkpoint_paths[0]}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Accumulate weights from subsequent checkpoints\n",
        "    for i in range(1, len(checkpoint_paths)):\n",
        "        try:\n",
        "            state_dict = torch.load(checkpoint_paths[i], map_location=DEVICE)\n",
        "            for k in avg_state_dict.keys():\n",
        "                avg_state_dict[k] += state_dict[k].to(dtype=torch.float32)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or adding checkpoint {checkpoint_paths[i]}: {e}. Skipping this checkpoint.\")\n",
        "            continue # Skip this checkpoint if it causes an error\n",
        "\n",
        "    # Average the accumulated weights\n",
        "    num_valid_checkpoints = len(checkpoint_paths) # Adjust if some were skipped due to errors\n",
        "    for k in avg_state_dict.keys():\n",
        "        avg_state_dict[k] /= num_valid_checkpoints\n",
        "\n",
        "    # Create a new model and load the averaged weights\n",
        "    averaged_model = get_model(NUM_CLASSES, pretrained=False) # Don't load timm pretrained weights\n",
        "    averaged_model.load_state_dict(avg_state_dict)\n",
        "    averaged_model.to(DEVICE)\n",
        "    averaged_model.eval()\n",
        "    print(\"Weight averaging complete.\")\n",
        "    return averaged_model\n",
        "\n",
        "def predict_with_tta(model, image_tensor): # image_tensor: (B, C, H, W)\n",
        "    model.eval()\n",
        "    image_tensor = image_tensor.to(DEVICE)\n",
        "\n",
        "    # Original prediction\n",
        "    with torch.no_grad():\n",
        "        outputs_original = model(image_tensor)\n",
        "        probs_original = torch.softmax(outputs_original, dim=1)\n",
        "\n",
        "    # Horizontal flip\n",
        "    flipped_image = transforms.functional.hflip(image_tensor)\n",
        "    with torch.no_grad():\n",
        "        outputs_flipped = model(flipped_image)\n",
        "        probs_flipped = torch.softmax(outputs_flipped, dim=1)\n",
        "\n",
        "    # Average probabilities (weight 0.5 for original, 0.5 for flipped)\n",
        "    # The prompt \"weight 0.5\" for TTA is a bit ambiguous.\n",
        "    # Common practice is (probs_original + probs_flipped) / 2\n",
        "    # Or if it means weight for the TTA part: (1-w)*orig + w*tta_aug\n",
        "    # Assuming equal weighting:\n",
        "    avg_probs = (probs_original + probs_flipped) / 2.0\n",
        "\n",
        "    return avg_probs\n",
        "\n",
        "\n",
        "# --- Part 5: TFLite Conversion (Can be run after fine-tuning) ---\n",
        "# (This part is similar to your previous script, adapted for the fine-tuned model)\n",
        "def convert_fine_tuned_to_tflite(model_path, num_classes, output_tflite_path=\"fine_tuned_seresnext.tflite\"):\n",
        "    try:\n",
        "        import ai_edge_torch # Import locally\n",
        "    except ImportError:\n",
        "        print(\"ai_edge_torch not installed. Skipping TFLite conversion.\")\n",
        "        print(\"Install with: pip install ai-edge-torch\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nConverting fine-tuned model {model_path} to TFLite...\")\n",
        "    model = get_model(num_classes, pretrained=False) # Load architecture, not pretrained weights\n",
        "\n",
        "    try:\n",
        "        # Load the fine-tuned weights\n",
        "        state_dict = torch.load(model_path, map_location=DEVICE)\n",
        "        # Handle potential DataParallel prefix 'module.'\n",
        "        if all(key.startswith('module.') for key in state_dict.keys()):\n",
        "            state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}\n",
        "        model.load_state_dict(state_dict)\n",
        "        model.eval()\n",
        "        model.to(DEVICE) # Ensure model is on the correct device before creating dummy input\n",
        "        print(\"Fine-tuned model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading fine-tuned model weights from {model_path}: {e}\")\n",
        "        return\n",
        "\n",
        "    # Prepare Dummy Input matching model's device\n",
        "    dummy_input = torch.randn(1, 3, INPUT_SIZE, INPUT_SIZE).to(DEVICE)\n",
        "    sample_inputs = (dummy_input,)\n",
        "\n",
        "    print(\"Converting model and exporting to TFLite...\")\n",
        "    try:\n",
        "        edge_model = ai_edge_torch.convert(model, sample_inputs)\n",
        "        print(\"Model converted to Edge format.\")\n",
        "        edge_model.export(output_tflite_path)\n",
        "        print(f\"TFLite model saved to {output_tflite_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during AI Edge Torch conversion or export: {e}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Download data\n",
        "    print(\"--- Starting Data Download ---\")\n",
        "    download_quickdraw_subset(QUICKDRAW_CATEGORIES, QUICKDRAW_BASE_URL, DOWNLOAD_DIR)\n",
        "\n",
        "    # 2. Prepare Dataloaders\n",
        "    print(\"\\n--- Preparing Dataloaders ---\")\n",
        "    train_loader, val_loader, class_to_idx = prepare_dataloaders(\n",
        "        QUICKDRAW_CATEGORIES, DOWNLOAD_DIR, VALIDATION_SAMPLES_PER_CLASS, BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    if train_loader and val_loader:\n",
        "        # 3. Get Model\n",
        "        print(\"\\n--- Initializing Model ---\")\n",
        "        model = get_model(NUM_CLASSES, pretrained=True)\n",
        "\n",
        "        # 4. Define Loss, Optimizer, Scheduler\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "        # Reduce LR when validation accuracy has stopped improving\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "        # 5. Train Model\n",
        "        print(\"\\n--- Starting Training ---\")\n",
        "        # Ensure checkpoint directory exists\n",
        "        if not os.path.exists(CHECKPOINT_DIR):\n",
        "            os.makedirs(CHECKPOINT_DIR)\n",
        "\n",
        "        recent_checkpoints = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, NUM_EPOCHS, CHECKPOINT_DIR, CHECKPOINT_SAVE_STEP)\n",
        "\n",
        "        # 6. (Optional) Average last K weights after training\n",
        "        if recent_checkpoints:\n",
        "            print(\"\\n--- Averaging Last K Weights (Example) ---\")\n",
        "            # For actual use, you might want to select the last 10 from all saved during training\n",
        "            # The `recent_checkpoints` list from `train_model` holds the last 10 saved due to step count.\n",
        "            # If you saved a 'best_model.pth', you might use that directly or average around it.\n",
        "\n",
        "            # Example: average checkpoints saved by step\n",
        "            # To get *all* step checkpoints:\n",
        "            all_step_checkpoints = sorted(glob.glob(os.path.join(CHECKPOINT_DIR, \"model_step_*.pth\")))\n",
        "            last_10_step_checkpoints = all_step_checkpoints[-10:]\n",
        "\n",
        "            if last_10_step_checkpoints:\n",
        "                 averaged_model = average_last_k_weights(last_10_step_checkpoints)\n",
        "                 if averaged_model:\n",
        "                     torch.save(averaged_model.state_dict(), os.path.join(CHECKPOINT_DIR, \"averaged_model.pth\"))\n",
        "                     print(f\"Averaged model saved to {os.path.join(CHECKPOINT_DIR, 'averaged_model.pth')}\")\n",
        "                     # Path for TFLite conversion could be this averaged model\n",
        "                     path_for_tflite = os.path.join(CHECKPOINT_DIR, \"averaged_model.pth\")\n",
        "            else:\n",
        "                print(\"Not enough step checkpoints to average. Using best_model.pth for TFLite conversion if available.\")\n",
        "                path_for_tflite = os.path.join(CHECKPOINT_DIR, \"best_model.pth\")\n",
        "        else:\n",
        "             path_for_tflite = os.path.join(CHECKPOINT_DIR, \"best_model.pth\")\n",
        "\n",
        "\n",
        "        # 7. Convert the fine-tuned model (e.g., best or averaged) to TFLite\n",
        "        if os.path.exists(path_for_tflite):\n",
        "            convert_fine_tuned_to_tflite(path_for_tflite, NUM_CLASSES)\n",
        "        else:\n",
        "            print(f\"Model file {path_for_tflite} not found for TFLite conversion.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Dataloaders could not be prepared. Exiting fine-tuning process.\")\n",
        "\n",
        "    print(\"\\n--- Script Finished ---\")\n"
      ],
      "metadata": {
        "id": "qiBr_piZ1f5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hEyog8tN4f5r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}